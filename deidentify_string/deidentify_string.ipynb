{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54394dd5-1f6f-49c6-9c80-4c0f9f279e2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Skyflow Deidentify String UDF\n",
    "\n",
    "This notebook will show you how to install a function for deidentifying strings and tokenizing PII in unstructured data using a Skyflow Vault."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de4c8638-b117-48ce-8c24-1d764466d9b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure Secrets (optional)\n",
    "\n",
    "To use this function as written you must configure a Secret in Databricks for storing the Skyflow API credentials. Alternately, for testing, when calling the function you can manually pass credentials as an argument.\n",
    "\n",
    "#### Install the CLI\n",
    "\n",
    "If you use Homebrew on a Mac, the below commands will complete the install.\n",
    "\n",
    "```\n",
    "brew tap databricks/tap\n",
    "brew install databricks\n",
    "```\n",
    "\n",
    "For more detailed instructions for any dev environment see the official documentation: [Databricks | Install or update the Databricks CLI](https://docs.databricks.com/aws/en/dev-tools/cli/install)\n",
    "\n",
    "##### Configure the CLI\n",
    "\n",
    "To get started and create a configuration profile on your machine, run `databricks configure`.\n",
    "\n",
    "You should be prompted for `Databricks Host` and a `Personal Access Token`. \n",
    "\n",
    "To get a Personal Access Token (PAT) for development login to the Databricks UI, open Settings, click Developer, then Access Tokens.\n",
    "\n",
    "For more information on authenticating the Databricks CLI see the official documentation: [Databricks | Authentication for the Databricks CLI](https://docs.databricks.com/aws/en/dev-tools/cli/authentication)\n",
    "\n",
    "#### Configure a secret scope in Databricks\n",
    "\n",
    "Now that you've configured and authenticated the Databricks CLI, run the following command to create a 'scope' for your secrets in Databricks: \n",
    "\n",
    "`databricks secrets create-scope <scope-name>`\n",
    "\n",
    "For the rest of this demo we'll use the scope `sky-agentic-demo`.\n",
    "\n",
    "`databricks secrets create-scope sky-agentic-demo`\n",
    "\n",
    "#### Get details from Skyflow\n",
    "\n",
    "- Create or log into your account at [skyflow.com](https://skyflow.com) and generate an API key: [docs.skyflow.com](https://docs.skyflow.com/api-authentication/)\n",
    "- Copy your API key, Vault URL, and Vault ID\n",
    "\n",
    "\n",
    "#### Store the secrets in Databricks\n",
    "\n",
    "Create your secrets using the JSON syntax:\n",
    "\n",
    "```sh\n",
    "databricks secrets put-secret --json '{\n",
    "  \"scope\": \"sky-agentic-demo\",\n",
    "  \"key\": \"sky_api_key\",\n",
    "  \"string_value\": \"--sky_api_key--\"\n",
    "}'\n",
    "```\n",
    "\n",
    "To confirm the secrets have been uploaded successfully, run `databricks secrets list-secrets sky-agentic-demo` to see a list of the keys you provided and an updated timestamp.\n",
    "\n",
    "Example:\n",
    "\n",
    "```sh\n",
    "Key            Last Updated Timestamp\n",
    "sky_api_key    1739998630197\n",
    "```\n",
    "\n",
    "Then to read a secret in a Notebook, use `dbutils.secrets`:\n",
    "\n",
    "`sky_api_key = dbutils.secrets.get(scope = \"sky-agentic-demo\", key = \"sky_api_key\")`\n",
    "\n",
    "To learn more about Secrets in Databricks, see the official documentation: [Secret Management | Databricks](https://docs.databricks.com/aws/en/security/secrets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc6dfba4-d3d5-4733-998d-cd3a79f01629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install the function\n",
    "\n",
    "Before you install, make sure you set your `vault_id` and `vault_url`. These are hardcoded values in our function, though you can modify it to also accept parameters for these values from the user invoking the function or use Databricks environment variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b63e5594-f0bf-4787-98d0-19cd5df878df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION\n",
    "agentic.default.deidentify_string (\n",
    " input_text STRING COMMENT 'The string to be de-identified.',\n",
    " sky_api_key STRING COMMENT 'The API key for the Skyflow API.'\n",
    ")\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "DETERMINISTIC\n",
    "COMMENT 'Deidentify a string using the Skyflow API. Removes any sensitive data from the string and returns a safe string with placeholders in place of sensitive data tokens.'\n",
    "AS $$\n",
    " import sys\n",
    " import json\n",
    " import requests\n",
    " from io import StringIO\n",
    " \n",
    " sys_stdout = sys.stdout\n",
    " redirected_output = StringIO()\n",
    " sys.stdout = redirected_output\n",
    "\n",
    " if sky_api_key is None or sky_api_key == '':\n",
    "     # try to fetch the API key from env variables\n",
    "     bearer_token = os.environ.get(\"SKY_API_KEY\")\n",
    " else:\n",
    "     bearer_token = sky_api_key\n",
    "\n",
    "-- SET YOUR VAULT ID\n",
    " vault_id = \"SKYFLOW_VAULT_ID\"\n",
    "-- SET YOUR VAULT URL\n",
    " vault_url = \"https://sample.vault.skyflowapis.com\"\n",
    "-- END\n",
    " api_path = \"/v1/detect/deidentify/string\"\n",
    " api_url = vault_url + api_path\n",
    " headers = {\n",
    "     \"Authorization\": f\"Bearer {bearer_token}\",\n",
    "     \"Content-Type\": \"application/json\",\n",
    " }\n",
    " json_body = {\n",
    "     \"vault_id\": vault_id,\n",
    "     \"text\": input_text,\n",
    " }\n",
    "\n",
    " try:\n",
    "     api_response = requests.post(api_url, headers=headers, json=json_body)\n",
    "     api_response.raise_for_status()\n",
    "     external_data = api_response.json()\n",
    "     result = external_data.get('processed_text', 'No processed_text found')\n",
    " except requests.exceptions.RequestException as e:\n",
    "     result = f\"Error calling external API: {str(e)}\"\n",
    "\n",
    " sys.stdout = sys_stdout\n",
    " return result\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ed1c2c2-0786-484b-bc82-102ece27fe76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the function from Unity Catalog\n",
    "\n",
    "Now that you've installed the deidentify_string() function into your Databricks Unity Catalog, you can call it from Python with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c9b0c76-fe35-403b-b3c7-ef0dd8388221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve an access token from Databricks Secrets.\n",
    "sky_api_key = dbutils.secrets.get(scope=\"sky-agentic-demo\", key=\"sky_api_key\")\n",
    "# Alternately, you can hardcode the API key here.\n",
    "# sky_api_key = \"yourkey\"\n",
    "\n",
    "# Provide some sample text. In practice you'll read this from a file or table.\n",
    "input_text = \"Hi my name is Joseph McCarron and I live in Austin TX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad60f995-78d3-48bb-bb3c-b8004ee5de50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>deidentified_text</th></tr></thead><tbody><tr><td>Hi my name is [NAME_1] and I live in [LOCATION_1]</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Hi my name is [NAME_1] and I live in [LOCATION_1]"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "deidentified_text",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the input dataframe\n",
    "df = spark.createDataFrame([(input_text,)], [\"input_text\"])\n",
    "df.createOrReplaceTempView(\"input_table\")\n",
    "\n",
    "# Create the result dataframe and pass the API key and the input dataframe\n",
    "result_df = spark.sql(f\"\"\"\n",
    "SELECT agentic.default.deidentify_string(input_text, '{sky_api_key}') AS deidentified_text\n",
    "FROM input_table\n",
    "\"\"\")\n",
    "\n",
    "# Display the result\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d0c7599-d442-4502-9294-0ea89f44bc42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "Now that you've created a basic string deidentification function you can try customizing it with some of the parameters available from the Skyflow Detect API. Or, if you're working with raw files, try creating a similar function for the Deidentify File APIs."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Skyflow deidentify_string() UDF",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
